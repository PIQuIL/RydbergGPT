{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "Machine learning has recently emerged as a powerful tool for predicting properties of quantum many-body systems. Generative models can learn from measurements of a single quantum state to accurately reconstruct the state and predict local observables for many ground states of Hamiltonians. In this tutorial, we focus on Rydberg atom systems and propose the use of conditional generative models to simultaneously represent a family of states by learning shared structures of different quantum states from measurements.\n",
    "\n",
    "Refs: \n",
    "\n",
    "[Predicting Properties of Quantum Systems with Conditional Generative Models](https://arxiv.org/abs/2211.16943)\n",
    "\n",
    "[Transformer Quantum State: A Multi-Purpose Model for Quantum Many-Body Problems](http://arxiv.org/abs/2208.01758)\n",
    "\n",
    "[Bloqade](https://queracomputing.github.io/Bloqade.jl/dev/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rydberg Hamiltonian \n",
    "\n",
    "\n",
    "We consider a system of $N=L \\times L$ atoms arranged on a square lattice.\n",
    "The governing Hamiltonian defining the Rydberg atom array interactions has the following form:\n",
    "\n",
    "$$\n",
    "\\hat{H} = \\sum_{i<j} \\frac{C_6}{\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\rVert^6} \\hat{n}_i \\hat{n}_j -\\delta \\sum_{i=1}^N \\hat{n}_i - \\frac{\\Omega}{2} \\sum_{i=1}^N \\hat{\\sigma}^x_i. \\quad (1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    C_6 = \\Omega \\left( \\frac{R_b}{a} \\right)^6, \\quad\n",
    "    V_{ij} =  \\frac{a^6}{\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\rVert^6}, \\quad (2)\n",
    "$$\n",
    "\n",
    "where $\\hat{\\sigma}^{x}_{i} = \\vert g \\rangle_i \\langle r\\vert_i + \\vert r \\rangle_i \\langle g\\vert_i $, the occupation number operator $\\hat{n}_i = \\frac{1}{2} \\left( \\hat{\\sigma}_{i} + \\mathbb{1} \\right) =  \\vert r\\rangle_i \\langle r \\vert_i$ and $\\hat{\\sigma}_{i} = \\vert r \\rangle_i \\langle r \\vert_i - \\vert g \\rangle_i \\langle g \\vert_i$. The experimental settings of a Rydberg atom array are controlled by the detuning from resonance $\\delta$, [Rabi frequency](https://en.wikipedia.org/wiki/Rabi_frequency#:~:text=The\\%20Rabi\\%20frequency\\%20is\\%20the,intensity) $\\Omega$, lattice length scale $a$ and the positions of the atoms $\\{\\mathbf{r}_i\\}_i^N$. From equation (2) above, we obtain a symmetric matrix $\\mathbf{V}$, that encapsulates the relevant information about the lattice geometry, and derive the Rydberg blockade radius $R_b$, within which simultaneous excitations are penalized. Finally, for the purposes of our study, the atom array is considered to be affected by thermal noise, in equilibrium at a temperature $T$. The experimental settings are thus captured by the set of parameters $\\mathbf{x} = (\\Omega, \\delta/\\Omega, R_b/a, \\mathbf{V}, \\beta / \\Omega)$, where $\\beta$ is the inverse temperature."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation of the quantum state\n",
    "Decomposing the joint distribution into a product of conditional distributions in an autoregressive manner,\n",
    "\n",
    "$$\n",
    " p_{\\theta}(\\boldsymbol{\\sigma}) = \\prod_{i=1}^n p_{\\theta}\\left(\\sigma_i \\mid \\sigma_{i-1}, \\ldots, \\sigma_1\\right).\n",
    "$$\n",
    "\n",
    "where $\\theta$ denotes the set of parameters of the generative model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Graph Encoder Decoder Transformer architecture\n",
    "\n",
    "In this tutorial, we will explain the network architecture used in the `get_rydberg_graph_encoder_decoder` function, which creates a RydbergEncoderDecoder model. This model is designed to process graph-structured data using a combination of Graph Convolutional Networks (GCNs) and the classic Encoder-Decoder architecture as introduced in [Vaswani et al.](https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch import Tensor, nn\n",
    "from torch_geometric.nn import GATConv, GCNConv\n",
    "\n",
    "from rydberggpt.models.graph_embedding.models import GraphEmbedding\n",
    "from rydberggpt.models.rydberg_encoder_decoder import RydbergEncoderDecoder\n",
    "\n",
    "from rydberggpt.models.transformer.layers import DecoderLayer, EncoderLayer\n",
    "from rydberggpt.models.transformer.models import (\n",
    "    Decoder,\n",
    "    Encoder,\n",
    "    EncoderDecoder,\n",
    "    Generator,\n",
    ")\n",
    "from rydberggpt.models.transformer.modules import (\n",
    "    PositionalEncoding,\n",
    "    PositionwiseFeedForward,\n",
    ")\n",
    "from rydberggpt.utils import to_one_hot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main components \n",
    "\n",
    "The `RydbergEncoderDecoder` model consists of the following main components:\n",
    "\n",
    "**Encoder:** The encoder processes the input graph data and generates a continuous representation. It consists of multiple `EncoderLayer` blocks, each containing a multi-head self-attention mechanism and a position-wise feed-forward network, followed by layer normalization and dropout.\n",
    "\n",
    "**Decoder:** The decoder takes the continuous representation generated by the encoder and produces the output predictions. It is composed of multiple `DecoderLayer` blocks, each containing two multi-head attention mechanisms (self-attention and encoder-decoder attention) and a position-wise feed-forward network, followed by layer normalization and dropout.\n",
    "\n",
    "**src_embed:** This component is responsible for transforming the input graph data into a continuous representation. It uses the `GraphEmbedding` class, which employs `GCNConv` layers (or other graph convolution layers, such as `GATConv`) to process the graph structure. The number of graph layers can be controlled with the `num_layers` parameter.\n",
    "\n",
    "**tgt_embed:** This is a sequential model that first applies a linear transformation to the target input states and then adds positional encoding to provide information about the sequence order. The positional encoding is applied using the `PositionalEncoding` class.\n",
    "\n",
    "**Generator:** The generator is a simple linear layer that maps the output of the decoder to the desired output dimension (in this case, 2). It is used for producing the final output predictions.\n",
    "\n",
    "In the `get_rydberg_graph_encoder_decoder` function, the model is created using the provided configuration (config). This configuration contains information about the model's dimensions, number of layers, and other hyperparameters. After initializing the model, the weights of the parameters with more than one dimension are initialized using [Xavier uniform initialization](https://paperswithcode.com/method/xavier-initialization).\n",
    "\n",
    "Overall, this network architecture combines the power of graph convolutional networks for processing graph-structured data with the sequence-to-sequence learning capabilities of the Encoder-Decoder architecture. This allows the model to effectively learn complex patterns in both the graph structure and the sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rydberg_graph_encoder_decoder(config):\n",
    "    c = copy.deepcopy\n",
    "    attn = nn.MultiheadAttention(config.d_model, config.num_heads, batch_first=True)\n",
    "    position = PositionalEncoding(config.d_model, config.dropout)\n",
    "    ff = PositionwiseFeedForward(config.d_model, config.d_ff, config.dropout)\n",
    "\n",
    "    model = RydbergEncoderDecoder(\n",
    "        encoder=Encoder(\n",
    "            EncoderLayer(config.d_model, c(attn), c(ff), config.dropout),\n",
    "            config.num_blocks_encoder,\n",
    "        ),\n",
    "        decoder=Decoder(\n",
    "            DecoderLayer(config.d_model, c(attn), c(attn), c(ff), config.dropout),\n",
    "            config.num_blocks_decoder,\n",
    "        ),\n",
    "        src_embed=GraphEmbedding(\n",
    "            graph_layer=GCNConv,  # GATConv\n",
    "            in_node_dim=config.in_node_dim,\n",
    "            d_hidden=config.graph_hidden_dim,\n",
    "            d_model=config.d_model,\n",
    "            num_layers=config.graph_num_layers,\n",
    "            dropout=config.dropout,\n",
    "        ),\n",
    "        tgt_embed=nn.Sequential(\n",
    "            nn.Linear(config.num_states, config.d_model), c(position)\n",
    "        ),\n",
    "        generator=Generator(config.d_model, 2),\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "The dataset is composed of $N_H$ Hamiltonians and obtain $N_s$ measurement outcomes for each ground state leading to a training set $\\mathcal{D}$  of size $N_HN_s$. The training objective is the average negative log-likelihood loss, \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) \\approx -\\frac{1}{|\\mathcal{D}|} \\sum_{\\boldsymbol{\\sigma} \\in \\mathcal{D}} \\ln p_{\\theta}(\\boldsymbol{\\sigma}).\n",
    "$$\n",
    "\n",
    "corresponding to maximizing the conditional likelihoods over the observed measurment outcomes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Embedding in Rydberg Atom Systems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In our approach, we leverage graph neural networks (GNNs) to process the underlying graph structure of Rydberg atom systems. In these systems, the graph nodes represent the Rydberg atoms, and each node is assigned a node_feature vector containing information about the Rabi frequency (Ω), detuning (Δ), and temperature (β). The Rydberg blockade radius, which determines the interaction strength between atoms, is encoded as edge attributes in the graph.\n",
    "\n",
    "GNNs are powerful tools for learning representations of graph-structured data, capturing both local and global information within the graph. In our model, we employ graph convolutional layers, such as GCNConv, to learn meaningful embeddings of the input graph. These embeddings take into account both node features and edge attributes, enabling the model to learn complex relationships between atoms in the Rydberg system.\n",
    "\n",
    "To understand the basics of graph neural networks and their applications, we recommend the following resources:\n",
    "\n",
    "1. [A Gentle Introduction to Graph Neural Networks](https://distill.pub/2021/gnn-intro/): This article provides an accessible and visually appealing introduction to GNNs, covering their motivation, core concepts, and various architectures.\n",
    "\n",
    "2. [Understanding Convolutions on Graphs](https://distill.pub/2021/understanding-gnns/): This article dives deeper into the inner workings of GNNs, specifically focusing on convolution operations on graphs. It provides insights into how graph convolutions can be understood as message-passing mechanisms and how they can be generalized.\n",
    "\n",
    "3. [Pytorch_geometric](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html): PyTorch Geometric is a library for deep learning on irregular input data such as graphs, point clouds, and manifolds. It provides efficient implementations of various GNN layers and models, making it easier to implement and experiment with graph-based neural networks. This resource serves as a guide to getting started with the library and provides documentation for its various features.\n",
    "\n",
    "In our Rydberg atom system model, the graph embedding component serves as a crucial bridge between the graph-structured input data and the encoder-decoder architecture. By leveraging the capabilities of GNNs, we can effectively learn complex patterns in the graph structure and enhance the performance of our model for predicting properties of quantum many-body systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
