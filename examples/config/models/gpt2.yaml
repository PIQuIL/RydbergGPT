transformer:
  num_heads: 8
  d_model: 128
  num_blocks: 8
  d_ff: 512 # usually 4 * d_model
  dropout: 0.1

training:
  devices: 3
  strategy: "ddp"
  accelerator: "gpu"
  precision: 16 # half precision saves lots of memory! 32 is default
  max_epochs: 20
  batch_size: 256 # 512 OOM for 3 devices. The batch size should not depend on ngpus. 
  learning_rate: 0.001
  optimizer: "Adam"
  criterion: "KLLoss"

dataset:
  num_atoms: null
  num_samples: null
  delta: null
  num_workers: 12 # suggestion seems to be 4 workers per GPU. Max is 32

rydberg:
  num_states: 2
  num_encoder_embedding_dims: 4

profiler: 
  profiler: "SimpleProfiler"

misc:
  seed: 42
  prog_bar: False