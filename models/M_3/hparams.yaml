accumulate_grad_batches: 1
advanced_monitoring: false
batch_size: 1024
chunks_in_memory: 200
compile: false
criterion: NLLLoss
d_ff: 128
d_model: 32
detect_anomaly: true
device: cuda
dropout: 0.1
eta_min: 1.0e-05
from_checkpoint: null
graph_hidden_dim: 64
graph_num_layers: 2
in_node_dim: 4
learning_rate: 0.001
log_every: 30
max_epochs: 1000
num_blocks_decoder: 3
num_blocks_encoder: 1
num_heads: 8
num_states: 2
num_workers_per_gpu: null
optimizer: AdamW
precision: 32
profiler: PyTorchProfiler
prog_bar: true
seed: 42
strategy: auto
t_initial: 1
t_mult: 2
