transformer:
  num_heads: 12
  d_model: 768
  num_blocks_encoder: 2
  num_blocks_decoder: 12
  d_ff: 2024 # usually 4 * d_model
  dropout: 0.1
  compile: False

graph_embedding:
  graph_num_layers: 3
  graph_hidden_dim: 128
  in_node_dim: 4 # omega delta beta, blockade_radius

training:
  strategy: "auto"
  precision: 32 # half precision saves lots of memory! 32 is default
  max_epochs: 20
  batch_size: 256 # 512 OOM for 3 devices. The batch size should not depend on ngpus. 
  learning_rate: 0.0005
  criterion: "NLLLoss" 
  from_checkpoint: null
  accumulate_grad_batches: 1
  detect_anomaly: True

logger:
  log_every: 30

optimizer:
  optimizer: "AdamW"
  t_initial: 1
  t_mult: 2
  eta_min: 0.00001

dataset:
  num_atoms: null
  num_samples: null
  delta: null
  num_workers: 12 # suggestion seems to be 4 workers per GPU. Max is 32

rydberg:
  num_states: 2
  num_encoder_embedding_dims: 3

profiler:
  profiler: "PyTorchProfiler" # https://pytorch-lightning.readthedocs.io/en/1.6.5/advanced/profiler.html

misc:
  seed: 42
  prog_bar: False
