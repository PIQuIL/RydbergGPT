{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Observables \n",
    "\n",
    "## Background\n",
    "\n",
    "In this tutorial, we are going to load a pretrained model, use it to generate new samples, and calculate relevant observables based on these samples.\n",
    "\n",
    "We consider a system of $N=L \\times L$ atoms arranged on a square lattice.\n",
    "The governing Hamiltonian defining the Rydberg atom array interactions has the following form:\n",
    "\n",
    "$$\n",
    "\\hat{H} = \\sum_{i<j} \\frac{C_6}{\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\rVert^6} \\hat{n}_i \\hat{n}_j -\\delta \\sum_{i=1}^N \\hat{n}_i - \\frac{\\Omega}{2} \\sum_{i=1}^N \\hat{\\sigma}^x_i. \\quad (1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    C_6 = \\Omega \\left( \\frac{R_b}{a} \\right)^6, \\quad\n",
    "    V_{ij} =  \\frac{a^6}{\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\rVert^6}, \\quad (2)\n",
    "$$\n",
    "\n",
    "where $\\hat{\\sigma}^{x}_{i} = \\vert g \\rangle_i \\langle r\\vert_i + \\vert r \\rangle_i \\langle g\\vert_i $, the occupation number operator $\\hat{n}_i = \\frac{1}{2} \\left( \\hat{\\sigma}_{i} + \\mathbb{1} \\right) =  \\vert r\\rangle_i \\langle r \\vert_i$ and $\\hat{\\sigma}_{i} = \\vert r \\rangle_i \\langle r \\vert_i - \\vert g \\rangle_i \\langle g \\vert_i$. The experimental settings of a Rydberg atom array are controlled by the detuning from resonance $\\delta$, Rabi frequency $\\Omega$, lattice length scale $a$ and the positions of the atoms $\\{\\mathbf{r}_i\\}_i^N$. From equation (2) above, we obtain a symmetric matrix $\\mathbf{V}$, that encapsulates the relevant information about the lattice geometry, and derive the Rydberg blockade radius $R_b$, within which simultaneous excitations are penalized. Finally, for the purposes of our study, the atom array is considered to be affected by thermal noise, in equilibrium at a temperature $T$. The experimental settings are thus captured by the set of parameters $\\mathbf{x} = (\\Omega, \\delta/\\Omega, R_b/a, \\mathbf{V}, \\beta / \\Omega)$, where $\\beta$ is the inverse temperature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from rydberggpt.models.rydberg_encoder_decoder import get_rydberg_graph_encoder_decoder\n",
    "from rydberggpt.models.utils import generate_prompt\n",
    "from rydberggpt.observables.rydberg_energy import (\n",
    "    get_rydberg_energy,\n",
    "    get_staggered_magnetization,\n",
    "    get_x_magnetization,\n",
    ")\n",
    "from rydberggpt.utils import create_config_from_yaml, load_yaml_file\n",
    "from rydberggpt.utils_ckpt import get_model_from_ckpt\n",
    "from torch_geometric.data import Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "\n",
    "We have three pretrained models trained on three different datasets. Model $M_1$ is trained with data for systems of size $L=5,6$ (`models/ds_1`); Model $M_2$ utilizes datasets for systems with $L=5,6,11,12$ (`models/ds_2`); and Model $M_3$ is trained on data covering  $L=5,6,11,12,15,16$ (`models/ds_3`).\n",
    "\n",
    "Let's start by loading the pretrained model and setting it into eval mode to ensure that the dropout layers are disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RydbergEncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0-1): 2 x SublayerConnection(\n",
       "            (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (src_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0-2): 3 x SublayerConnection(\n",
       "            (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (src_embed): GraphEmbedding(\n",
       "    (layers): ModuleList(\n",
       "      (0): GraphLayer(\n",
       "        (graph_layer): GCNConv(4, 64)\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): GCNConv(64, 32)\n",
       "    )\n",
       "    (final_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "base_path = os.path.abspath(\"../\")\n",
    "log_path = os.path.join(base_path, \"models/ds_1/\")\n",
    "\n",
    "yaml_dict = load_yaml_file(log_path, \"hparams.yaml\")\n",
    "config = create_config_from_yaml(yaml_dict)\n",
    "\n",
    "model = get_model_from_ckpt(\n",
    "    log_path, model=get_rydberg_graph_encoder_decoder(config), ckpt=\"best\"\n",
    ")\n",
    "model.to(device=device)\n",
    "model.eval()  # don't forget to set to eval mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating system prompt and samples\n",
    "\n",
    "Next, let us define our system prompt $\\mathbf{x} = (\\Omega, \\delta/\\Omega, R_b/a, \\mathbf{V}, \\beta / \\Omega)$. Below is a function `generate_prompt` that generates the required prompt structure to query the trained model. The prompt is a graph structure capturing the relevant information about the system, such as the lattice geometry, the Rydberg blockade radius, the temperature, and the Rabi frequency. The function `generate_samples` generates samples from the model given the prompt.\n",
    "\n",
    "### System prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 5\n",
    "delta = 1.0\n",
    "omega = 1.0\n",
    "beta = 64.0\n",
    "Rb = 1.15\n",
    "num_samples = 5\n",
    "\n",
    "pyg_graph = generate_prompt(\n",
    "    model_config=config,\n",
    "    n_rows=L,\n",
    "    n_cols=L,\n",
    "    delta=delta,\n",
    "    omega=omega,\n",
    "    beta=beta,\n",
    "    Rb=Rb,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating samples\n",
    "\n",
    "The sampling function requires a batch of prompts; therefore, we duplicate our `pyg_graph` prompts as many times as we want to generate samples. The reasoning behind this is to allow the model to generate samples in parallel for different Hamiltonian parameters. This is especially helpful when training variationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating atom 25/25                                                          \n"
     ]
    }
   ],
   "source": [
    "# duplicate the prompt for num_samples\n",
    "cond = [pyg_graph for _ in range(num_samples)]\n",
    "cond = Batch.from_data_list(cond)\n",
    "\n",
    "samples = model.get_samples(\n",
    "    batch_size=len(cond), cond=cond, num_atoms=L**2, fmt_onehot=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observables \n",
    "\n",
    "Now we are ready to calculate observables based on the samples generated. We consider three observables: the stagger-magnetization, x-magnetization and the Rydberg energy.\n",
    "### Rydberg energy \n",
    "\n",
    "We consider an estimate of the ground state energy $\\langle E \\rangle$, which is defined as\n",
    "\n",
    "$$\n",
    "\\langle E \\rangle  \n",
    "\\approx \\frac{1}{N_s} \\sum_{\\boldsymbol{\\sigma} \\sim p_{\\theta}(\\boldsymbol{\\sigma};\\mathbf{x})} \\frac{\\langle \\boldsymbol{\\sigma}|\\widehat{H}|\\Psi_{\\theta}\\rangle}{\\langle \\boldsymbol{\\sigma}|\\Psi_{\\theta}\\rangle}.\n",
    "$$\n",
    "\n",
    "We provide a function `get_rydberg_energy` that calculates the Rydberg energy of the samples generated. Note that this fn requires a single prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0244)\n"
     ]
    }
   ],
   "source": [
    "energy = get_rydberg_energy(model, samples, cond=pyg_graph, device=device)\n",
    "print(energy.mean() / L**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stagger magnetization\n",
    "\n",
    "The staggered magnetization for the square-lattice Rydberg array defined in its occupation basis.  This quantity is the order parameter for the disorder-to-checkerboard quantum phase transition, and can be calculated simply with \n",
    "\n",
    "$$\n",
    "    \\langle\\hat{\\sigma}^{\\text{stag}}\\rangle  \\approx \\frac{1}{N_s} \\sum_{\\boldsymbol{\\sigma} \\sim p_\\theta(\\boldsymbol{\\sigma};\\mathbf{x})}\n",
    "    \\left|   \\sum_{i=1}^{N} (-1)^i  \\frac{n_i(\\boldsymbol{\\sigma}) - 1/2}{N} \\right| ,\n",
    "$$\n",
    "\n",
    "where $i$ runs over all $N = L \\times L$ atoms and $n_i(\\boldsymbol{\\sigma}) = \\langle \\boldsymbol{\\sigma}| r_i \\rangle\\langle r_i|\\boldsymbol{\\sigma} \\rangle$ is the occupation number operator acting on atom $i$ in a given configuration $\\boldsymbol{\\sigma}$. \n",
    "Because this observable is diagonal, it can be computed directly from samples inferred from the decoder. The outer sum shows how importance sampling is used to estimate the expectation value over this operator, approximating the probability of a given configuration with the frequency with which it is sampled. \n",
    "\n",
    "\n",
    "We provide a function `get_staggered_magnetization` that calculates the stagger magnetization of the samples generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0144)\n"
     ]
    }
   ],
   "source": [
    "staggered_magnetization = get_staggered_magnetization(samples, L, L, device=device)\n",
    "print(staggered_magnetization.mean() / L**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X-magnetization\n",
    "\n",
    "We consider an off-diagonal observable, where we must make use of the ground state wave function amplitudes of the inferred samples $\\Psi(\\boldsymbol{\\sigma}) = \\sqrt{p_{\\theta}(\\boldsymbol{\\sigma})}$. \n",
    "As an example, we examine the spatially averaged expectation value of $\\hat{\\sigma}_x$, which is defined as\n",
    "\n",
    "$$\n",
    "    \\langle \\hat{\\sigma}^x \\rangle  \\approx \\frac{1}{N_s} \\sum_{\\boldsymbol{\\sigma} \\sim p_\\theta(\\boldsymbol{\\sigma};\\mathbf{x})} \\frac{1}{N}\n",
    "    \\sum_{\\boldsymbol{\\sigma}' \\in \\mathrm{SSF}(\\boldsymbol{\\sigma})} \\frac{\\Psi_\\theta(\\boldsymbol{\\sigma}')}{\\Psi_\\theta(\\boldsymbol{\\sigma})},\n",
    "$$\n",
    "\n",
    "where the variable $\\left\\{\\boldsymbol{\\sigma'}\\right\\}$ is the set of configurations that are connected to $\\boldsymbol{\\sigma}$ by a single spin flip (SSF).\n",
    "\n",
    "We provide a function `get_x_magnetization` that calculates the stagger magnetization of the samples generated.\n",
    "Note that we do not have to batch our prompt. The energy is calculated for a single system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7578)\n"
     ]
    }
   ],
   "source": [
    "x_magnetization = get_x_magnetization(model, samples, cond=pyg_graph, device=device)\n",
    "print(x_magnetization.mean() / L**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rydberggpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
